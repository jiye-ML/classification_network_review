* [paper](paper/40.003-17-Residual-Attention-Network-for-Image-Classification.pdf)

### 动机

* 深度学习中的Attention，源自于人脑的注意力机制，当人的大脑接受到外部信息，如视觉信息、听觉信息时，往往不会对全部信息进行处理和理解，而只会将注意力集中在部分显著或者感兴趣的信息上，这样有助于滤除不重要的信息，而提升信息处理的效率。
* 最早将Attention利用在图像处理上的出发点是，希望通过一个类似于人脑注意力的机制，只利用一个很小的感受野去处理图像中Attention的部分，降低了计算的维度。
* 而后来慢慢的，有人发现其实卷积神经网络自带Attention的功能，比方说在分类任务中，高层的feature map所激活的pixel也恰好集中在与分类任务相关的区域，也就是salience map，常被用在图像检测和分割上。那么如何利用Attention来提升模型在分类任务上的性能呢？本文提供了一种新的思路。

### 以前的方法

* 在图片分类中，自上而下的注意力机制应用不同方法
  * 序列处理：model问题为序列决策
  * 区域推荐：
  * 控制门：LSTM

### 创新

* 提出了一种可堆叠的网络结构。与ResNet中的Residual Block类似，本文所提出的网络结构也是通过一个Residual  Attention Module的结构进行堆叠，可使网络模型能够很容易的达到很深的层次。
* 提出了一种基于Attention的残差学习方式。与ResNet也一样，本文做提出的模型也是通过一种残差的方式，使得非常深的模型能够容易的优化和学习，并且具有非常好的性能。
* Bottom-up Top-down的前向Attention机制。其他利用Attention的网络，往往需要在原有网络的基础上新增一个分支来提取Attention，并进行单独的训练，而本文提出的模型能够就在一个前向过程中就提取模型的Attention，使得模型训练更加简单。

### 优势

* 利用mask分支学习特征权值：可以一直在更新trunk分支权值的时候错误的梯度，对噪声更鲁棒；

### 方法

* 下图基本上可以囊括本位的绝大部分内容，对于某一层的输出feature map，也就是下一层的输入，对于一个普通的网络，只有右半部分，也就是Trunk Branch，作者在这个基础上增加了左半部分：Soft Mask Branch——一个Bottom-up Top-down的结构。

![这里写图片描述](readme/40.003-创新.png)

* Bottom-up Top-down的结构首先通过一系列的卷积和pooling，逐渐提取高层特征并增大模型的感受野，之前说过高层特征中所激活的Pixel能够反映Attention所在的区域，于是再通过相同数量的up sample将feature map的尺寸放大到与原始输入一样大，这样就将Attention的区域对应到输入的每一个pixel上，我们称之为Attention map。Bottom-up Top-down这种encoder-decoder的结构在图像分割中用的比较多，如FCN，也正好是利用了这种结构相当于一个weakly-supervised的定位任务的学习。
* 接下来就要把Soft Mask Branch与Trunk Branch的输出结合起来，Soft Mask Branch输出的Attention map中的每一个pixel值相当于对原始feature map上每一个pixel值的权重，它会增强有意义的特征，而抑制无意义的信息，因此，将Soft Mask Branch与Trunk Branch输出的feature map进行element-wised的乘法，就得到了一个weighted Attention map。但是无法直接将这个weighted Attention map输入到下一层中，因为Soft Mask Branch的激活函数是Sigmoid，输出值在（0，1）之间（之所以这么做，我认为是不希望给前后两层的feature map带来太大的差异和扰动，其次能够进一步的抑制不重要的信息），因此通过一系列这样的乘法，将会导致feature map的值越来越小，并且也可能打破原始网络的特性，当层次极深时，给训练带来了很大的困难。因此作者在得到了weighted Attention map之后又与原来Trunk Branch的feature map进行了一个element-wised的操作，这就和ResNet有异曲同工之妙，该层的输出由下面这个式子组成： 

![这里写图片描述](readme/40.003-公式.png)

* 其中M(x)为Soft Mask Branch的输出，F(x)为Trunk Branch的输出，那么当M(x)=0时，该层的输入就等于F(x)，因此该层的效果不可能比原始的F(x)差，这一点也借鉴了ResNet中恒等映射的思想，同时这样的加法，也使得Trunk Branch输出的feature map中显著的特征更加显著，增加了特征的判别性。这样，优化的问题解决了，性能的问题也解决了，因此通过将这种残差结构进行堆叠，就能够很容易的将模型的深度达到很深的层次，具有非常好的性能。

![1542943469450](readme/40.003-结构.png)

* 上图是利用Residual Attention Module进行堆叠得到的模型结构，三个stage，由浅到深提取不同层次的Attention信息，值得注意的是，网络中的每一个unit都可以换成目前具有非常好性能的结构，如Residual Block、Inception Block，换个角度说，就是可以将这个Attention的结构无缝连接到目前最优秀的网络中去，使得模型的性能更上一层楼。 

* 下图是一个使用在ResNet-50上的例子，可以看出来和原始的ResNet的区别就是在每个阶段的Residual Block之间增加了Attention Module，这里有一个小trick，就是在每一个Attention Module的Soft Mask Branch中，作者使得down sample到的最小feature map的尺寸与整个网络中的最小feature map大小一致，首先7x7的feature map对于Attention来说不至于那么粗糙，作者不希望在浅层的Attention丢失的信息太多，其次也保证了它们具有相同大小的感受野。至于这么做到底能提升多大的性能，这里也很难判定。

![这里写图片描述](readme/40.003-结构_02.png)

### 实验

* 作者在ImageNet数据集上与ResNet、Inception-ResNet等一系列当下最优秀的方法进行了比较：

![这里写图片描述](readme/40.003-实验.png)

* 作者使用了不同的Attention unit，得到了结果也比原始的网络有不少的提升，这也有力的证明了Attention的效果，以及作者这种Residual Attention学习的有效性。

### 总结

* 将已有的结构当做是积木，搭建你自己系统的时候如果需要，或者可以用上，就要拿来，解决问题而不是创造积木。看论文的时候，不仅要知道它是如何使用，更要了解它的本质和思路，并能够作为己用，这才能最大的提升学习的效果。





