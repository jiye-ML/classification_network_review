* [大话CNN经典模型：GoogLeNet（从Inception v1到v4的演进）](https://my.oschina.net/u/876354/blog/1637819)
    * 一般来说，提升网络性能最直接的办法就是增加网络深度和宽度，深度指网络层次数量、宽度指神经元数量。但这种方式存在以下问题：
        1. 参数太多，如果训练数据集有限，很容易产生过拟合；
        2. 网络越大、参数越多，计算复杂度越大，难以应用；
        3. 网络越深，容易出现梯度弥散问题（梯度越往后穿越容易消失），难以优化模型。
    * GoogLeNet采用了模块化的结构（Inception结构），方便增添和修改；
    * 为了避免梯度消失，网络额外增了2个辅助的softmax用于向前传导梯度（辅助分类器）。
    辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，
    同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，
    这两个额外的softmax会被去掉。(也可以防止梯度长距离传播带来的爆炸或者消失);
    * Inception V3一个最重要的改进是分解（Factorization），将7x7分解成两个一维的卷积（1x7,7x1），
    3x3也是一样（1x3,3x1），这样的好处，既可以加速计算，又可以将1个卷积拆成2个卷积，使得网络深度进一步增加，
    增加了网络的非线性（每增加一层都要进行ReLU）。另外，网络输入从224x224变为了299x299。
    * Inception V4研究了Inception模块与残差连接的结合。ResNet结构大大地加深了网络深度，还极大地提升了训练速度，
    同时性能也有提升
    