## 2016-Wide residual networks.

* [paper](paper/30.103-16-Wide-residual-networks.pdf)
* [[DL-架构-ResNet系] 004 WRN](https://zhuanlan.zhihu.com/p/29681673)
* [WRNS：Wide Residual Networks 论文笔记](https://blog.csdn.net/wspba/article/details/72229177)

### 总结

* 宽度说的是每层特征图的数量；
* 变宽说的是增加每层特征图的数量；
 
### 动机

* 适当的增加ResNet中block的宽度比增加网络深度可以更有效的提升性能 ，这说明残差网络的能力主要由残差block提供，网络深度只有补充性的作用。
* 作者认为，随着模型深度的加深，梯度反向传播时，并不能保证能够流经每一个残差模块（residual block）的weights，以至于它很难学到东西，因此在整个训练过程中，只有很少的几个残差模块能够学到有用的表达，而绝大多数的残差模块起到的作用并不大。因此作者希望使用一种较浅的，但是宽度更宽的模型，来更加有效的提升模型的性能；
* 我们16层的宽度残差网络准确率相当于1000层的深度残差网络的;

### 我们贡献

* 我们提出了残余网络架构的详细实验研究，彻底检查了ResNet块结构的几个重要方面。
* 我们提出宽度结构的残差模块，提升模型性能
* 我们提出在深度残差网络中使用dropout的新方法；

### 网络结构

* 各种残差结构，我们在这个基础上加宽网络。

  ![1540382083414](../readme/30.103-各种残差结构.png)

* 宽度网络的结构

  ![1540382507839](../readme/30.103-网络结构.png)

### 实验结果

* 几个关键参数 WRN-40-2-B(3,3)
  * 40：表示有40个卷积层
  * 2：表示每个残差模块宽度 * 2
  * B(3, 3)：每个残差模块由两个`3*3`的卷积组成，

* 不同k下对比：

  ![1540451559827](../readme/30.103-不同网络架构实验.png)

* 我们的网络与其他网络对比：

  ![1540453277305](../readme/30.103-与不同网络结果对比.png)

* 深度与宽度的对比：

  ![1540453715885](../readme/30.103-深度与宽度的对比.png)

* 加入dropout后的对比：

  ![1540453765025](../readme/30.103-加入dropout实验.png)

* 时间上的对比：

  ![1540453990099](../readme/30.103-时间上的对比.png)