---
typora-copy-images-to: readme
---

# 分类网络

### 几篇综述

* [深度学习之四大经典CNN技术浅析 | 硬创公开课](https://www.leiphone.com/news/201702/dgpHuriVJHTPqqtT.html)
    * AlexNet:
        * 拥有5个卷积层，其中3个卷积层后面连接了最大池化层，最后还有3个全连接层。
    * VGGNet :
        * 全部利用3*3的卷积和2*2的池化，取代大的卷积核。
* [The 9 Deep Learning Papers You Need To Know About](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
* [如何评价 Squeeze-and-Excitation Networks ?](https://www.zhihu.com/question/65044831/answer/227262160)



## 0. 数据处理

[github: DL-Data](https://github.com/jiye-ML/DL-Data)
* 当你训练一个机器学习模型时，你实际做工作的是调参，以便将特定的输入（一副图像）映像到输出（标签）。
我们优化的目标是使模型的损失最小化， 以正确的方式调节优化参数即可实现这一目标。



## 1. AlexNet

* [paper](paper/2012-AlexNet.pdf)

![AlexNet 网络架构](readme/AlexNet_网络架构.png)
![AlexNet 网络参数](readme/AlexNet每层的超参数及参数数量.jpg)

### 1.1 核心观点

* 在ImageNet数据集上训练网络, 15 million 标注图片 超过 22,000 类别.

### AlexNet主要使用到的新技术点如下。

1. 成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。
虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。
2. 训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，
通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。
3. 在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。
并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。
5. 提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。
6. 数据增强，随机地从256´256的原始图像中截取224´224大小的区域（以及水平翻转的镜像），
相当于增加了(256-224)2´2=2048倍的数据量。如果没有数据增强，仅靠原始的数据量，参数众多的CNN会陷入过拟合中，
使用了数据增强后可以大大减轻过拟合，提升泛化能力。进行预测时，则是取图片的四个角加中间共5个位置，并进行左右翻转，
一共获得10张图片，对他们进行预测并对10次结果求均值。同时，AlexNet论文中提到了会对图像的RGB数据进行PCA处理，
并对主成分做一个标准差为0.1的高斯扰动，增加一些噪声，这个Trick可以让错误率再下降1%。

* 使用批随机梯度下降算法训练网络, 使用特殊值作为动量和权值衰减。
* Trained on two GTX 580 GPUs 5/6天。


## 2. VGG Net

* [paper](paper/2014-Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition.pdf)
    * 网络结构 \
    ![](readme/vgg_01.png)
    * 核心观点
        * 仅仅使用3x3 卷积核，作者认为组合2个 3x3 卷积层和5x5核具有一样的感受野。这使得卷积核尺寸表现，感受野不变。
        一个好处是减少了参数量。
        * 随着输入空间尺寸的变小，通道数不断加大
        * 每次池化之后都是用多个卷积层，这诠释了空间尺度缩小的时候，深度加深。
        * 在训练过程中使用scale jittering作为一种数据增强技术。
        * 使用每个卷积层后使用ReLU层并且通过 batch gradient descent训练。
    * 比其他网络表现力更强，与gooleNet相比，网络架构更简单。
    * 3*3网络的优势：
        1. 每一个卷积后面接ReLU，这样更加非线性
        2. 参数量少 3个3x3的卷积相当于一个7x7的卷积，感受野相同。
    * 可以试着实现一些trick
        1. momentum SGD 0.9
        2. weight decay L2 * 5e-4
        3. drop out 0.5
        4. learning rate = 1e-2, 当验证集准确度不在提升时，衰减10， 衰减3次
        5. 初始化方式，预训练
    * 初始化策略
        * 可以在一个小的网络上训练然后初始化
        * 训练11层的网络，然后利用11层的网络初始化相应的16层的网络
        * 可以利用256的数据训练，然后初始化384数据的网络
    
* [大话CNN经典模型：VGGNet](https://my.oschina.net/u/876354/blog/1634322)



 


























