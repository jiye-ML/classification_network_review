## 4. ResNet

![](paper/resnet_01.jpg)

* [大话深度残差网络（DRN）ResNet网络原理](https://my.oschina.net/u/876354/blog/1622896)



## 《Identity mappings in deep residual networks》

* [paper](paper/2016-Identity%20Mappings%20in%20Deep%20Residual%20Networks.pdf)
* [【一个神经元统治一切】ResNet 强大的理论证明](https://mp.weixin.qq.com/s/xTJr-jWMjk73TCZ8gBT4Ww)
* [何恺明CVPR演讲：深入理解ResNet和视觉识别的表示学习（41 PPT）](https://mp.weixin.qq.com/s/Wmj0dkHA93RCWB10lUcgmw)



##《Deep Residual Learning for Image Recognition 》

* [paper](paper/2015-Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf)
* [github code](ResNet.py)

### 论文阅读

* 仅仅加深深度在普通的网络上的表现

  ![1537975506631](readme/ResNet_普通网络加深后的表现.png)

* 获得更好的网络的办法是队列更多的层吗？回答这个问题的一个障碍是消失/爆炸梯度的臭名昭着的问题，从一开始就阻碍了融合。

* 残差网络模块

![resnet_block](readme/resnet_block_01.png)

* 学习函数的不同

  ![1540084975075](readme/resnet_学习函数_01.png)

* **为什么恒等映射**：退化问题表明求解器可能难以通过多个非线性层逼近恒等映射。 利用残差学习重构，如果恒等映射是最优的，则求解器可以简单地将多个非线性层的权重推向零以接近恒等映射。

  ![1540086191220](readme/resnet_恒等映射_01.png)

* 对于其他形式的映射关系，不需要

  ![1540086846270](readme/resnet_恒等映射_02.png)





## 《Densely Connected Convolutional Networks》
* [paper](paper/2018-Densely%20Connected%20Convolutional%20Networks.pdf)

### 优点：

#### 1. 减少了梯度消失问题，增强了特征图，鼓励特征的重用；

* 发现在训练过程中的每一步都随机地扔掉（drop）一些层，可以显著的提高 ResNet 的泛化性能。这个方法的成功带来以下两点启发：
  1. 它说明了神经网络其实并不一定要是一个递进层级结构，也就是说网络中的某一层可以不仅仅依赖于紧邻的上一层的特征，还可以依赖于更前面层学习的特征。想像一下在随机深度网络中，当第 l 层被扔掉后，第 L+1 层就被直接连到了第 L-1 层；当第 2 到了第 L 层都被扔掉之后，第 L+1 层就直接用到了第 1 层的特征。因此，随机深度网络其实可以看成一个具有随机密集连接的 DenseNet。
  2. 在训练的过程中随机扔掉很多层也不会破坏算法的收敛，说明了 ResNet 具有比较明显的冗余性，网络中的每一层都只提取了很少的特征（即所谓的残差）。实际上，将训练好的 ResNet 随机的去掉几层，对网络的预测结果也不会产生太大的影响。既然每一层学习的特征这么少，能不能降低它的计算量来减小冗余呢？

- DenseNet 的设计正是基于以上两点观察。让网络中的每一层都直接与其前面层相连，实现特征的重复利用；同时把网络的每一层设计得特别窄，即只学习非常少的特征图（最极端情况就是每一层只学习一个特征图），达到降低冗余性的目的。这两点也是 DenseNet 与其他网络最主要的不同。需要强调的是，第一点是第二点的前提，没有密集连接，就不可能把网络设计得太窄，否则训练会出现欠拟合现象，即使 ResNet 也是如此。

![1540131200939](readme/DenseNet_优点_01.png)

#### 2. 计算量和参数量少：

- 达到相同效果下，densenet参数量1/3的ResNet

![1540172961933](readme/DenseNet_优点_03.png)

![1540172889619](readme/DenseNet_优点_02.png)

#### 3. densenet提高准确率：

* 单独的层接受了额外的附加的来自跳跃连接损失函数的监督信息;

* densenet实现了相似的监督信息：一个单一的分类的器提供直接的监督信息给所有的层，然而损失函数和梯度不复杂，损失函数在所有层共享。

### 网络结构

* 网络结构：不是ResNet一样，相加相融合，我们采用堆叠的方式融合特征。

  ![DenseNet_01](readme/DenseNet_block.png)

* ResNets的一个优点是可以直接将梯度从后层传向前层。然而，自身与经过转换得到的输出是通过求和的形式来连接的，这可能使网络中信息的传播受到影响。

  ![1540133245522](readme/DenseNet_输出_01.png)

* 网络的整体结构，因为有了pooling层，会使得输出尺寸变小；

  ![1540133371187](readme/DenseNet_整体架构.png)

  ![1540134434924](readme/DenseNet_整体架构_02.png)

### 实验结果：

![1540134527513](readme/DenseNet_实验结果.png)







## SENet 《Squeeze-and-Excitation Networks》

- SENet是基于特征通道之间的关系提出的，下图是SENet的Block单元，图中的Ftr是传统的卷积结构，X和U是Ftr的输入和输出，这些都是以往结构中已存在的。SENet增加的部分是U后的结构：对U先做一个Global Average Pooling（称为Squeeze过程），输出是一个1x1xC的数据，再经过两级全连接（称为Excitation过程），最后用sigmoid把输出限制到[0，1]的范围，把这个值作为scale再乘到U的C个通道上，作为下一级的输入数据。这种结构的原理是想**通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。**![SENet_block](readme/SENet_block.png)
- 下图是把SENet模型分别用于Inception网络和ResNet网络，下图左边部分是原始网络，右边部分是加了SENet之后的网络，分别变成SE-Inception和SE-ResNet。网络中的r是压缩参数，先通过第一个全连接层把1x1xC的数据压缩为1x1xC/r，再通过第二个全连接层把数据扩展到1x1xC。 

![SENet_inception](readme/SENet_inception.png)

