### [v2] Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift

* [paper](paper/21.02-00-2015-Batch%20Normalization%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift.pdf)
    
* BN本质上解决的是反向传播过程中的梯度问题。
    

    
* [《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》论文解读](https://www.cnblogs.com/dmzhuo/p/5889157.html)
    
    1. 归一化：
        * 原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；
        * 一旦每批训练数据的分布各不相同，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度；
    2. 传递性
        * 网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。
        * 一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，
        训练数据的分布一直在发生变化，那么将会影响网络的训练速度。
    
* [基础 | batchnorm原理及代码详解](https://blog.csdn.net/qq_25737169/article/details/79048516)
    
    * 每层输出的均值和归一化处理
