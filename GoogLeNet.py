import timeimport osimport zipfileimport numpy as npimport shutilimport globimport scipy.misc as miscimport tensorflow as tfimport argparseimport tensorflow.contrib.slim as slimclass Tools:    def __init__(self):        pass    @staticmethod    def print_info(info):        print(time.strftime("%H:%M:%S", time.localtime()), info)        pass    # 新建目录    @staticmethod    def new_dir(path):        if not os.path.exists(path):            os.makedirs(path)        return path    passclass PreData:    def __init__(self, zip_file, ratio=4):        data_path = zip_file.split(".zip")[0]        self.train_path = os.path.join(data_path, "train")        self.test_path = os.path.join(data_path, "test")        if not os.path.exists(data_path):            f = zipfile.ZipFile(zip_file, "r")            f.extractall(data_path)            all_image = self.get_all_images(os.path.join(data_path, data_path.split("/")[-1]))            self.get_data_result(all_image, ratio, Tools.new_dir(self.train_path), Tools.new_dir(self.test_path))        else:            Tools.print_info("data is exists")        pass    # 生成测试集和训练集    @staticmethod    def get_data_result(all_image, ratio, train_path, test_path):        train_list = []        test_list = []        # 遍历        Tools.print_info("bian")        for now_type in range(len(all_image)):            now_images = all_image[now_type]            for now_image in now_images:                # 划分                if np.random.randint(0, ratio) == 0:  # 测试数据                    test_list.append((now_type, now_image))                else:                    train_list.append((now_type, now_image))            pass        # 打乱        Tools.print_info("shuffle")        np.random.shuffle(train_list)        np.random.shuffle(test_list)        # 提取训练图片和标签        Tools.print_info("train")        for index in range(len(train_list)):            now_type, image = train_list[index]            shutil.copyfile(image, os.path.join(train_path,                                                str(np.random.randint(0, 1000000)) + "-" + str(now_type) + ".jpg"))        # 提取测试图片和标签        Tools.print_info("test")        for index in range(len(test_list)):            now_type, image = test_list[index]            shutil.copyfile(image, os.path.join(test_path,                                                str(np.random.randint(0, 1000000)) + "-" + str(now_type) + ".jpg"))        pass    # 所有的图片    @staticmethod    def get_all_images(images_path):        all_image = []        all_path = os.listdir(images_path)        for one_type_path in all_path:            now_path = os.path.join(images_path, one_type_path)            if os.path.isdir(now_path):                now_images = glob.glob(os.path.join(now_path, "*.jpg"))                all_image.append(now_images)            pass        return all_image    # 生成数据    @staticmethod    def main(zip_file):        pre_data = PreData(zip_file)        return pre_data.train_path, pre_data.test_path    passclass Data:    def __init__(self, batch_size, type_number, image_size, image_channel, train_path, test_path):        self.batch_size = batch_size        self.type_number = type_number        self.image_size = image_size        self.image_channel = image_channel        self._train_images = glob.glob(os.path.join(train_path, "*.jpg"))        self._test_images = glob.glob(os.path.join(test_path, "*.jpg"))        self.test_batch_number = len(self._test_images) // self.batch_size        pass    def next_train(self):        begin = np.random.randint(0, len(self._train_images) - self.batch_size)        return self._norm_image_label(self._train_images[begin: begin + self.batch_size])    def next_test(self, batch_count):        begin = self.batch_size * (0 if batch_count >= self.test_batch_number else batch_count)        return self._norm_image_label(self._test_images[begin: begin + self.batch_size])    @staticmethod    def _norm_image_label(images_list):        images = [np.array(misc.imread(image_path).astype(np.float)) / 255.0 for image_path in images_list]        labels = [int(image_path.split("-")[1].split(".")[0]) for image_path in images_list]        return images, labels    passclass InceptionNet:    def __init__(self, type_number, image_size, image_channel, batch_size):        self._type_number = type_number        self._image_size = image_size        self._image_channel = image_channel        self._batch_size = batch_size        pass    @staticmethod    def _inception_v3_arg_scope(weight_decay=0.00004, stddev=0.1, batch_norm_var_collection="moving_vars"):        batch_norm_params = {            "decay": 0.9997,            "epsilon": 0.001,            "updates_collections": tf.GraphKeys.UPDATE_OPS,            "variables_collections": {                "beta": None,                "gamma": None,                "moving_mean": [batch_norm_var_collection],                "moving_variance": [batch_norm_var_collection]            }        }        # slim.arg_scope 可以给函数的参数自动赋予某些默认值        with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay)):            with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm,                                weights_initializer=tf.truncated_normal_initializer(stddev=stddev),                                normalizer_params=batch_norm_params) as sc:                return sc        pass    @staticmethod    def _inception_v3_base(inputs, scope):        end_points = {}        # 299        with tf.variable_scope(scope, values=[inputs]):            # 非Inception Module:5个卷积层和2个最大池化层            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding="VALID"):                with tf.variable_scope("group_non"):                    net = slim.conv2d(inputs, 32, [3, 3], stride=2)  # 149 X 149 X 32                    net = slim.conv2d(net, 32, [3, 3])  # 147 X 147 X 32                    net = slim.conv2d(net, 64, [3, 3], padding="SAME")  # 147 X 147 X 64                    net = slim.max_pool2d(net, [3, 3], stride=2)  # 73 X 73 X 64                    net = slim.conv2d(net, 80, [1, 1])  # 73 X 73 X 80                    net = slim.conv2d(net, 192, [3, 3])  # 71 X 71 X 192                    net = slim.max_pool2d(net, [3, 3], stride=2)  # 35 X 35 X 192                pass            # 非Inception Module的结果            end_points["group_non"] = net            # 共有3个连续的Inception模块组            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding="SAME"):                # 第1个模块组包含了3个Inception Module                with tf.variable_scope("group_1a"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 64, [1, 1])  # 35 X 35 X 64                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 48, [1, 1])                        branch_1 = slim.conv2d(branch_1, 64, [5, 5])  # 35 X 35 X 64                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 64, [1, 1])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])  # 35 X 35 X 96                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 32, [1, 1])  # 35 X 35 X 32                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 35 X 35 X 256                    pass                with tf.variable_scope("group_1b"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 64, [1, 1])  # 35 X 35 X 64                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 48, [1, 1])                        branch_1 = slim.conv2d(branch_1, 64, [5, 5])  # 35 X 35 X 64                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 64, [1, 1])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])  # 35 X 35 X 96                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 64, [1, 1])  # 35 X 35 X 64                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 35 X 35 X 288                    pass                with tf.variable_scope("group_1c"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 64, [1, 1])  # 35 X 35 X 64                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 48, [1, 1])                        branch_1 = slim.conv2d(branch_1, 64, [5, 5])  # 35 X 35 X 64                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 64, [1, 1])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])  # 35 X 35 X 96                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 64, [1, 1])  # 35 X 35 X 64                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 35 X 35 X 288                    pass                # 第1个模块组的结果                end_points["group_1c"] = net  # 35 X 35 X 288                # 第2个模块组包含了5个Inception Module                with tf.variable_scope("group_2a"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 384, [3, 3], stride=2, padding="VALID")  # 17 X 17 X 384                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 64, [1, 1])                        branch_1 = slim.conv2d(branch_1, 96, [3, 3])                        branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2, padding="VALID")  # 17 X 17 X 96                    with tf.variable_scope("branch_2"):                        branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding="VALID")  # 17 X 17 X 288                    net = tf.concat([branch_0, branch_1, branch_2], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2b"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1], padding="VALID")  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 128, [1, 1])                        branch_1 = slim.conv2d(branch_1, 128, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 128, [1, 1])                        branch_2 = slim.conv2d(branch_2, 128, [7, 1])                        branch_2 = slim.conv2d(branch_2, 128, [1, 7])                        branch_2 = slim.conv2d(branch_2, 128, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2c"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 160, [1, 1])                        branch_1 = slim.conv2d(branch_1, 160, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 160, [1, 1])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 160, [1, 7])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2d"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 160, [1, 1],)                        branch_1 = slim.conv2d(branch_1, 160, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 160, [1, 1])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 160, [1, 7])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2e"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 160, [1, 1])                        branch_1 = slim.conv2d(branch_1, 160, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 160, [1, 1])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 160, [1, 7])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                # 第2个模块组的结果                end_points["group_2e"] = net  # 17 X 17 X 768                # 第3个模块组包含了3个Inception Module                with tf.variable_scope("group_3a"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])                        branch_0 = slim.conv2d(branch_0, 320, [3, 3], stride=2, padding="VALID")  # 8 X 8 X 320                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 192, [1, 1])                        branch_1 = slim.conv2d(branch_1, 192, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])                        branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2, padding="VALID")  # 8 X 8 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding="VALID")  # 8 X 8 X 768                    net = tf.concat([branch_0, branch_1, branch_2], axis=3)  # 8 X 8 X 1280                    pass                with tf.variable_scope("group_3b"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 320, [1, 1])  # 8 X 8 X 320                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 384, [1, 1])                        branch_1 = tf.concat([slim.conv2d(branch_1, 384, [1, 3]),                                              slim.conv2d(branch_1, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 448, [1, 1])                        branch_2 = slim.conv2d(branch_2, 384, [3, 3])                        branch_2 = tf.concat([slim.conv2d(branch_2, 384, [1, 3]),                                              slim.conv2d(branch_2, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 8 X 8 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 8 X 8 X 2048                    pass                with tf.variable_scope("group_3c"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 320, [1, 1])  # 8 X 8 X 320                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 384, [1, 1])                        branch_1 = tf.concat([slim.conv2d(branch_1, 384, [1, 3]),                                              slim.conv2d(branch_1, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 448, [1, 1])                        branch_2 = slim.conv2d(branch_2, 384, [3, 3])                        branch_2 = tf.concat([slim.conv2d(branch_2, 384, [1, 3]),                                              slim.conv2d(branch_2, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 8 X 8 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 8 X 8 X 2048                    pass                # 第3个模块组的结果                end_points["group_3c"] = net  # 8 X 8 X 2048            pass        return net, end_points    # 网络    # keep_prob=0.8    def inception_v3(self, input_op, is_training=True, reuse=None, **kw):        with tf.variable_scope("inception_v3", values=[input_op, self._type_number], reuse=reuse) as scope:            # slim.arg_scope 可以给函数的参数自动赋予某些默认值            with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):                net, end_points = self._inception_v3_base(inputs=input_op, scope=scope)                with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding="SAME"):                    # 辅助分类节点：Auxiliary Logits，将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类                    # 结果中，相当于做了模型的融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化。                    with tf.variable_scope("aux_logits"):                        aux_logits = end_points["group_2e"]  # 17 X 17 X 768                        aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3, padding="VALID")  # 5 X 5 X 768                        aux_logits = slim.conv2d(aux_logits, 128, [1, 1])  # 5 X 5 X 128                        # 299                        # aux_logits = slim.conv2d(aux_logits, 768, [5, 5], padding="VALID",  # 1 X 1 X 768                        #                          weights_initializer=tf.truncated_normal_initializer(stddev=0.01))                        # 256                        aux_logits = slim.conv2d(aux_logits, 768, [4, 4], padding="VALID",  # 1 X 1 X 768                                                 weights_initializer=tf.truncated_normal_initializer(stddev=0.01))                        aux_logits = slim.conv2d(aux_logits, self._type_number, [1, 1],                                                 activation_fn=None, normalizer_fn=None,   # 1 X 1 X num_classes                                                 weights_initializer=tf.truncated_normal_initializer(stddev=0.001))                        aux_logits = tf.squeeze(aux_logits, [1, 2])  # num_classes                        end_points["aux_logits"] = aux_logits                        pass                    # 正常的Logits                    with tf.variable_scope("logits"):                        # 299                        # net = slim.avg_pool2d(net, [8, 8], padding="VALID")  # 1 X 1 X 2048                        # 256                        net = slim.avg_pool2d(net, [6, 6], padding="VALID")  # 1 X 1 X 2048                        net = slim.dropout(net, keep_prob=kw["keep_prob"])                        end_points["pre_logits"] = net                        logits = slim.conv2d(net, self._type_number, [1, 1],   # 1 X 1 X num_classes                                             activation_fn=None, normalizer_fn=None)                        logits = tf.squeeze(logits, [1, 2])  # num_classes                        end_points["logits"] = logits                        pass                    pass                softmax = slim.softmax(logits)                end_points["softmax"] = softmax                end_points["prediction"] = tf.argmax(softmax, 1)            pass        return logits, end_points["softmax"], end_points["prediction"]    passclass Runner:    def __init__(self, data, classifies, learning_rate, **kw):        self._data = data        self._type_number = self._data.type_number        self._image_size = self._data.image_size        self._image_channel = self._data.image_channel        self._batch_size = self._data.batch_size        self._classifies = classifies        input_shape = [self._batch_size, self._image_size, self._image_size, self._image_channel]        self._images = tf.placeholder(shape=input_shape, dtype=tf.float32)        self._labels = tf.placeholder(dtype=tf.int32, shape=[self._batch_size])        self._logits, self._softmax, self._prediction = classifies(self._images, **kw)        self._entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self._labels, logits=self._logits)        self._loss = tf.reduce_mean(self._entropy)        self._solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(self._loss)        self._saver = tf.train.Saver()        self._sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))        pass    # 训练网络    def train(self, epochs, save_model, min_loss, print_loss, test, save):        self._sess.run(tf.global_variables_initializer())        epoch = 0        for epoch in range(epochs):            images, labels = self._data.next_train()            loss, _ = self._sess.run([self._loss, self._solver], feed_dict={self._images: images, self._labels: labels})            if epoch % print_loss == 0:                Tools.print_info("{}: loss {}".format(epoch, loss))            if loss < min_loss:                break            if epoch % test == 0:                self.test()                pass            if epoch % save == 0:                self._saver.save(self._sess, save_path=save_model)            pass        Tools.print_info("{}: train end".format(epoch))        self.test()        Tools.print_info("test end")        pass    # 测试网络    def test(self):        all_ok = 0        test_epoch = self._data.test_batch_number        for now in range(test_epoch):            images, labels = self._data.next_test(now)            prediction = self._sess.run(fetches=self._prediction, feed_dict={self._images: images})            all_ok += np.sum(np.equal(labels, prediction))        all_number = test_epoch * self._batch_size        Tools.print_info("the result is {} ({}/{})".format(all_ok / (all_number * 1.0), all_ok, all_number))        pass    passif __name__ == '__main__':    parser = argparse.ArgumentParser()    parser.add_argument("-name", type=str, default="AlexNet", help="name")    parser.add_argument("-epochs", type=int, default=2, help="train epoch number") # 10    parser.add_argument("-batch_size", type=int, default=2, help="batch size") # 32    parser.add_argument("-type_number", type=int, default=45, help="type number")    parser.add_argument("-image_size", type=int, default=256, help="image size")    parser.add_argument("-image_channel", type=int, default=3, help="image channel")    parser.add_argument("-keep_prob", type=float, default=0.7, help="keep prob")    parser.add_argument("-zip_file", type=str, default="data/resisc45.zip", help="zip file path")    args = parser.parse_args()    output_param = "name={},epochs={},batch_size={},type_number={},image_size={},image_channel={},zip_file={},keep_prob={}"    Tools.print_info(output_param.format(args.name, args.epochs, args.batch_size, args.type_number,                                         args.image_size, args.image_channel, args.zip_file, args.keep_prob))    now_train_path, now_test_path = PreData.main(zip_file=args.zip_file)    now_data = Data(batch_size=args.batch_size, type_number=args.type_number, image_size=args.image_size,                    image_channel=args.image_channel, train_path=now_train_path, test_path=now_test_path)    now_net = InceptionNet(now_data.type_number, now_data.image_size, now_data.image_channel, now_data.batch_size)    runner = Runner(data=now_data, classifies=now_net.inception_v3, learning_rate=0.0001, keep_prob=args.keep_prob)    runner.train(epochs=args.epochs, save_model=Tools.new_dir("model/" + args.name) + args.name + ".ckpt",                 min_loss=1e-4, print_loss=200, test=1000, save=10000)    pass