import tensorflow as tfimport tensorflow.contrib.slim as slimclass GoogLeNet:    def __init__(self, type_number, image_size, image_channel, batch_size):        self._type_number = type_number        self._image_size = image_size        self._image_channel = image_channel        self._batch_size = batch_size        pass    def _inception_v3_arg_scope(self, weight_decay=0.00004, stddev=0.1, batch_norm_var_collection="moving_vars"):        batch_norm_params = {            "decay": 0.9997,            "epsilon": 0.001,            "updates_collections": tf.GraphKeys.UPDATE_OPS,            "variables_collections": {                "beta": None,                "gamma": None,                "moving_mean": [batch_norm_var_collection],                "moving_variance": [batch_norm_var_collection]            }        }        # slim.arg_scope 可以给函数的参数自动赋予某些默认值        with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay)):            with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm,                                weights_initializer=tf.truncated_normal_initializer(stddev=stddev),                                normalizer_params=batch_norm_params) as sc:                return sc        pass    def _inception_v3_base(self, inputs, scope):        end_points = {}        with tf.variable_scope(scope, values=[inputs]):            # 非Inception Module:5个卷积层和2个最大池化层            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding="VALID"):                with tf.variable_scope("group_non"):                    net = slim.conv2d(inputs, 32, [3, 3], stride=2)  # 149 X 149 X 32                    net = slim.conv2d(net, 32, [3, 3])  # 147 X 147 X 32                    net = slim.conv2d(net, 64, [3, 3], padding="SAME")  # 147 X 147 X 64                    net = slim.max_pool2d(net, [3, 3], stride=2)  # 73 X 73 X 64                    net = slim.conv2d(net, 80, [1, 1])  # 73 X 73 X 80                    net = slim.conv2d(net, 192, [3, 3])  # 71 X 71 X 192                    net = slim.max_pool2d(net, [3, 3], stride=2)  # 35 X 35 X 192                pass            # 非Inception Module的结果            end_points["group_non"] = net            # 共有3个连续的Inception模块组            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding="SAME"):                # 第1个模块组包含了3个Inception Module                with tf.variable_scope("group_1a"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 64, [1, 1])  # 35 X 35 X 64                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 48, [1, 1])                        branch_1 = slim.conv2d(branch_1, 64, [5, 5])  # 35 X 35 X 64                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 64, [1, 1])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])  # 35 X 35 X 96                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 32, [1, 1])  # 35 X 35 X 32                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 35 X 35 X 256                    pass                with tf.variable_scope("group_1b"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 64, [1, 1])  # 35 X 35 X 64                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 48, [1, 1])                        branch_1 = slim.conv2d(branch_1, 64, [5, 5])  # 35 X 35 X 64                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 64, [1, 1])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])  # 35 X 35 X 96                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 64, [1, 1])  # 35 X 35 X 64                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 35 X 35 X 288                    pass                with tf.variable_scope("group_1c"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 64, [1, 1])  # 35 X 35 X 64                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 48, [1, 1])                        branch_1 = slim.conv2d(branch_1, 64, [5, 5])  # 35 X 35 X 64                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 64, [1, 1])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])                        branch_2 = slim.conv2d(branch_2, 96, [3, 3])  # 35 X 35 X 96                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 64, [1, 1])  # 35 X 35 X 64                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 35 X 35 X 288                    pass                # 第1个模块组的结果                end_points["group_1c"] = net  # 35 X 35 X 288                # 第2个模块组包含了5个Inception Module                with tf.variable_scope("group_2a"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 384, [3, 3], stride=2, padding="VALID")  # 17 X 17 X 384                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 64, [1, 1])                        branch_1 = slim.conv2d(branch_1, 96, [3, 3])                        branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2, padding="VALID")  # 17 X 17 X 96                    with tf.variable_scope("branch_2"):                        branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding="VALID")  # 17 X 17 X 288                    net = tf.concat([branch_0, branch_1, branch_2], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2b"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1], padding="VALID")  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 128, [1, 1])                        branch_1 = slim.conv2d(branch_1, 128, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 128, [1, 1])                        branch_2 = slim.conv2d(branch_2, 128, [7, 1])                        branch_2 = slim.conv2d(branch_2, 128, [1, 7])                        branch_2 = slim.conv2d(branch_2, 128, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2c"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 160, [1, 1])                        branch_1 = slim.conv2d(branch_1, 160, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 160, [1, 1])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 160, [1, 7])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2d"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 160, [1, 1],)                        branch_1 = slim.conv2d(branch_1, 160, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 160, [1, 1])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 160, [1, 7])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                with tf.variable_scope("group_2e"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 160, [1, 1])                        branch_1 = slim.conv2d(branch_1, 160, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])  # 17 X 17 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 160, [1, 1])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 160, [1, 7])                        branch_2 = slim.conv2d(branch_2, 160, [7, 1])                        branch_2 = slim.conv2d(branch_2, 192, [1, 7])  # 17 X 17 X 192                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 17 X 17 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 17 X 17 X 768                    pass                # 第2个模块组的结果                end_points["group_2e"] = net  # 17 X 17 X 768                # 第3个模块组包含了3个Inception Module                with tf.variable_scope("group_3a"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 192, [1, 1])                        branch_0 = slim.conv2d(branch_0, 320, [3, 3], stride=2, padding="VALID")  # 8 X 8 X 320                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 192, [1, 1])                        branch_1 = slim.conv2d(branch_1, 192, [1, 7])                        branch_1 = slim.conv2d(branch_1, 192, [7, 1])                        branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2, padding="VALID")  # 8 X 8 X 192                    with tf.variable_scope("branch_2"):                        branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding="VALID")  # 8 X 8 X 768                    net = tf.concat([branch_0, branch_1, branch_2], axis=3)  # 8 X 8 X 1280                    pass                with tf.variable_scope("group_3b"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 320, [1, 1])  # 8 X 8 X 320                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 384, [1, 1])                        branch_1 = tf.concat([slim.conv2d(branch_1, 384, [1, 3]),                                              slim.conv2d(branch_1, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 448, [1, 1])                        branch_2 = slim.conv2d(branch_2, 384, [3, 3])                        branch_2 = tf.concat([slim.conv2d(branch_2, 384, [1, 3]),                                              slim.conv2d(branch_2, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 8 X 8 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 8 X 8 X 2048                    pass                with tf.variable_scope("group_3c"):                    with tf.variable_scope("branch_0"):                        branch_0 = slim.conv2d(net, 320, [1, 1])  # 8 X 8 X 320                    with tf.variable_scope("branch_1"):                        branch_1 = slim.conv2d(net, 384, [1, 1])                        branch_1 = tf.concat([slim.conv2d(branch_1, 384, [1, 3]),                                              slim.conv2d(branch_1, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_2"):                        branch_2 = slim.conv2d(net, 448, [1, 1])                        branch_2 = slim.conv2d(branch_2, 384, [3, 3])                        branch_2 = tf.concat([slim.conv2d(branch_2, 384, [1, 3]),                                              slim.conv2d(branch_2, 384, [3, 1])], 3)  # 8 X 8 X 768                    with tf.variable_scope("branch_3"):                        branch_3 = slim.avg_pool2d(net, [3, 3])                        branch_3 = slim.conv2d(branch_3, 192, [1, 1])  # 8 X 8 X 192                    net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=3)  # 8 X 8 X 2048                    pass                # 第3个模块组的结果                end_points["group_3c"] = net  # 8 X 8 X 2048            pass        return net, end_points    # 网络    # keep_prob=0.8    def inception_v3(self, input_op, is_training=True, reuse=None, **kw):        with tf.variable_scope("inception_v3", values=[input_op, self._type_number], reuse=reuse) as scope:            # slim.arg_scope 可以给函数的参数自动赋予某些默认值            with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):                net, end_points = self._inception_v3_base(inputs=input_op, scope=scope)                with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding="SAME"):                    # 辅助分类节点：Auxiliary Logits，将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类                    # 结果中，相当于做了模型的融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化。                    with tf.variable_scope("aux_logits"):                        aux_logits = end_points["group_2e"]  # 17 X 17 X 768                        aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3, padding="VALID")  # 5 X 5 X 768                        aux_logits = slim.conv2d(aux_logits, 128, [1, 1])  # 5 X 5 X 128                        # 299                        # aux_logits = slim.conv2d(aux_logits, 768, [5, 5], padding="VALID",  # 1 X 1 X 768                        #                          weights_initializer=tf.truncated_normal_initializer(stddev=0.01))                        # 256                        aux_logits = slim.conv2d(aux_logits, 768, [4, 4], padding="VALID",  # 1 X 1 X 768                                                 weights_initializer=tf.truncated_normal_initializer(stddev=0.01))                        aux_logits = slim.conv2d(aux_logits, self._type_number, [1, 1],                                                 activation_fn=None, normalizer_fn=None,   # 1 X 1 X num_classes                                                 weights_initializer=tf.truncated_normal_initializer(stddev=0.001))                        aux_logits = tf.squeeze(aux_logits, [1, 2])  # num_classes                        end_points["aux_logits"] = aux_logits                        pass                    # 正常的Logits                    with tf.variable_scope("logits"):                        # 299                        # net = slim.avg_pool2d(net, [8, 8], padding="VALID")  # 1 X 1 X 2048                        # 256                        net = slim.avg_pool2d(net, [6, 6], padding="VALID")  # 1 X 1 X 2048                        net = slim.dropout(net, keep_prob=kw["keep_prob"])                        end_points["pre_logits"] = net                        logits = slim.conv2d(net, self._type_number, [1, 1],   # 1 X 1 X num_classes                                             activation_fn=None, normalizer_fn=None)                        logits = tf.squeeze(logits, [1, 2])  # num_classes                        end_points["logits"] = logits                        pass                    pass                softmax = slim.softmax(logits)                end_points["softmax"] = softmax                end_points["prediction"] = tf.argmax(softmax, 1)            pass        return logits, end_points["softmax"], end_points["prediction"]    def fit(self,  input_op, is_training=True, reuse=None, version="v3", **kw):        if version == "v3":            return self.inception_v3(input_op, is_training, reuse, kw)    pass